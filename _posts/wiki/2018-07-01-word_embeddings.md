---
title: Word Embeddings
layout: post
base: Wiki
base_url: /wiki
hidden: true
---


Git
---

Source code for my attempts: [<https://github.com/Tzeny/udacity-deep-learning/blob/master/5_word2vec.ipynb>](https://github.com/Tzeny/udacity-deep-learning/blob/master/5_word2vec.ipynb)

In the git repository above I've also included the embeddings matrix resulted from the trained in the form of a .pickle file.

Example predictions
-------------------

Some examples from the skip-gram training:

` Nearest to one: two, four, seven, eight, three, six, five, nine,`
` Nearest to man: person, woman, boy, chanute, glasgow, programmatical, trudeau, revenge,`
` Nearest to concept: form, idea, locational, testimony, result, definition, tordesillas, nisos,`
